

一个Server若要向SMgr注册自己Binder就必需通过0这个引用号和SMgr的Binder通信


先搞明白Binder是如何跨进程的

然后再搞明白 两个进程各拿着一个Binder如何通信

进程A如何拿到进程B的Binder ServiceManager

所有的Binder都保存在ServiceManager里
客户端向ServiceManager查询其他Binder的时候，也是进程间通信。
但是我们知道，ServiceManager有一个编号为0的Binder，直接拿，然后就可以和ServiceManager通信了。

经过查询之后，Binder就有两个引用了，一个位于SMgr中，一个位于发起请求的Client中。

showProxyFields

定时开关机 改成 开关机设置

有关Linux的内存管理知识，可以参考Android学习启动篇一文提到的《Understanding the Linux Kernel》一书中的第8章。

  这里为什么会同时使用进程虚拟地址空间和内核虚拟地址空间来映射同一个物理页面呢？这就是Binder进程间通信机制的精髓所在了，
        同一个物理页面，一方映射到进程虚拟地址空间，一方面映射到内核虚拟地址空间，这样，进程和内核之间就可以减少一次内存拷贝了，提到了进程间通信效率。
Client进程空间的数据拷贝一次到内核空间，然后Server与内核共享这个数据就可以了，整个过程只需要执行一次内存拷贝，提高了效率。



Android系统进程间通信（IPC）机制Binder中的Server启动过程源代码分析

这里有一个非常重要的地方，是Binder进程间通信机制的精髓所在：
[cpp] view plain copy 在CODE上查看代码片派生到我的代码片
tr.data.ptr.buffer = (void *)t->buffer->data + proc->user_buffer_offset;
tr.data.ptr.offsets = tr.data.ptr.buffer + ALIGN(t->buffer->data_size, sizeof(void *));
   t->buffer->data所指向的地址是内核空间的，现在要把数据返回给Service Manager进程的用户空间，
   而Service Manager进程的用户空间是不能访问内核空间的数据的，所以这里要作一下处理。怎么处理呢？我们在学面向对象语言的时候，
   对象的拷贝有深拷贝和浅拷贝之分，深拷贝是把另外分配一块新内存，然后把原始对象的内容搬过去，浅拷贝是并没有为新对象分配一块新空间，
   而只是分配一个引用，而个引用指向原始对象。Binder机制用的是类似浅拷贝的方法，通过在用户空间分配一个虚拟地址，
   然后让这个用户空间虚拟地址与 t->buffer->data这个内核空间虚拟地址指向同一个物理地址，这样就可以实现浅拷贝了。
   怎么样用户空间和内核空间的虚拟地址同时指向同一个物理地址呢？请参考前面一篇文章浅谈Service Manager成为Android进程间通信（IPC）
   机制Binder守护进程之路，那里有详细描述。这里只要将t->buffer->data加上一个偏移
值proc->user_buffer_offset就可以得到t->buffer->data对应的用户空间虚拟地址了。调整了tr.data.ptr.buffer的值之后，不要忘记也要一起调整tr.data.ptr.offsets的值。



/frameworks/base/core/java/com/android/internal/os/BinderInternal.java

BinderInternal的getContextObject方法是一个JNI方法
它定义在
/frameworks/base/core/jni/android_util_Binder.cpp
如下
```
static jobject android_os_BinderInternal_getContextObject(JNIEnv* env, jobject clazz){
    sp<IBinder> b = ProcessState::self()->getContextObject(NULL);
    return javaObjectForIBinder(env, b);
}
```
然后ProcessState的getContextObject方法内部调用getStrongProxyForHandle(0)，注意参数传了0
再内部调用lookupHandleLocked方法
```
ProcessState::handle_entry* ProcessState::lookupHandleLocked(int32_t handle)
{
    const size_t  N= mHandleToObject.size();
    if (N <= (size_t)handle) {
        handle_entry e;
        e.binder = NULL;
        e.refs = NULL;
        status_t err = mHandleToObject.insertAt(e, N, handle+1-N);
        if (err < NO_ERROR) return NULL;
    }
    return &mHandleToObject.editItemAt(handle);
}
```

Vector<handle_entry> mHandleToObject;
这些IBinder都存在这个Vector的里 查找完之后返回
这个handle的编号是什么，是在Vector里的位置么？

ActivityManagerNative内的 gDefault有如下代码
IBinder b = ServiceManager.getService("activity");
它最终赋值给了 mRemote

然后在这个类的startService方法里调用了
mRemote.transact(START_SERVICE_TRANSACTION, data, reply, 0);

 现在，由于要把这个Binder实体MediaPlayerService交给target_proc，也就是Service Manager来管理，也就是说Service Manager要引用这个MediaPlayerService了

 这里有一个非常重要的地方，是Binder进程间通信机制的精髓所在：

 BpBinder的transact内部调用了IPCThreadState的transact方法

IPCThreadState的transact方法内部先writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL);
把数据写到mOut里

然后调用了waiForResponse在这个函数里调用了talkWithDriver
这个函数用来和驱动交互 进行读写操作
ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr)
进入到binder


我们在另一个进程拿不到SM的实体，所以只能通过它的代理来做。

svclist

今晚研究 mmap是如何进行内存映射的
然后研究copy_from_user方法
参照gityuan博客驱动篇
拿着白纸比划着

写清楚的话，应该先介绍几个结构体，然后再往下讲。

其中伙伴算法和slab高速缓存都在物理内存映射区分配物理内存，而vmalloc机制则在高端内存映射区分配物理内存。

内核地址来个偏移，就映射到用户空间去了。
alloc_pages()和alloc_page()

缺页异常
另外，事实上，在每个进程创建加载时，内核只是为进程“创建”了虚拟内存的布局，具体就是初始化进程控制表中内存相关的链表，
实际上并不立即就把虚拟内存对应位置的程序数据和代码（比如.text .data段）拷贝到物理内存中，
只是建立好虚拟内存和磁盘文件之间的映射就好（叫做存储器映射），等到运行到对应的程序时，才会通过缺页异常，来拷贝数据。
还有进程运行过程中，要动态分配内存，比如malloc时，也只是分配了虚拟内存，即为这块虚拟内存对应的页表项做相应设置，
当进程真正访问到此数据时，才引发缺页异常。


可以认为虚拟空间都被映射到了磁盘空间中，（事实上也是按需要映射到磁盘空间上，通过mmap），
并且由页表记录映射位置，当访问到某个地址的时候，通过页表中的有效位，可以得知此数据是否在内存中，
如果不是，则通过缺页异常，将磁盘对应的数据拷贝到内存中，如果没有空闲内存，则选择牺牲页面，替换其他页面。

mmap是用来建立从虚拟空间到磁盘空间的映射的，可以将一个虚拟空间地址映射到一个磁盘文件上，
当不设置这个地址时，则由系统自动设置，函数返回对应的内存地址（虚拟地址），
当访问这个地址的时候，就需要把磁盘上的内容拷贝到内存了，
然后就可以读或者写，最后通过manmap可以将内存上的数据换回到磁盘，
也就是解除虚拟空间和内存空间的映射，这也是一种读写磁盘文件的方法，也是一种进程共享数据的方法 共享内存




IA32架构中内核虚拟地址空间只有1GB大小（从3GB到4GB），因此可以直接将1GB大小的物理内存（即常规内存）映射到内核地址空间，
但超出1GB大小的物理内存（即高端内存）就不能映射到内核空间。为此，内核采取了下面的方法使得内核可以使用所有的物理内存。
1).高端内存不能全部映射到内核空间，也就是说这些物理内存没有对应的线性地址。
不过，内核为每个物理页框都分配了对应的页框描述符，所有的页框描述符都保存在mem_map数组中，
因此每个页框描述符的线性地址都是固定存在的。内核此时可以使用alloc_pages()和alloc_page()来分配高端内存，
因为这些函数返回页框描述符的线性地址。
 2).内核地址空间的后128MB专门用于映射高端内存，否则，没有线性地址的高端内存不能被内核所访问。
 这些高端内存的内核映射显然是暂时映射的，否则也只能映射128MB的高端内存。
 当内核需要访问高端内存时就临时在这个区域进行地址映射，使用完毕之后再用来进行其他高端内存的映射。
由于要进行高端内存的内核映射，因此直接能够映射的物理内存大小只有896MB，该值保存在high_memory中。内核地址空间的线性地址区间如下图所示：
